# ── LLM Gateway Configuration ──────────────────────────────────
# All settings use the LLM_ prefix and are read automatically.

# Provider: "anthropic" | "local_claude" | "openai" (future)
LLM_PROVIDER=anthropic

# Model identifier (provider-specific)
LLM_MODEL=claude-sonnet-4-5-20250514

# API key — if not set, falls back to ANTHROPIC_API_KEY / OPENAI_API_KEY
# LLM_API_KEY=sk-ant-...

# Optional base URL override (for proxies or custom endpoints)
# LLM_BASE_URL=https://api.anthropic.com

# Max tokens per response
LLM_MAX_TOKENS=4096

# Retry settings
LLM_MAX_RETRIES=3
LLM_TIMEOUT_SECONDS=120

# ── Cost Guardrails ─────────────────────────────────────────────
# Set to limit cumulative cost per LLMClient instance (USD)
# LLM_COST_LIMIT_USD=10.0
# LLM_COST_WARN_USD=5.0

# ── Observability ───────────────────────────────────────────────
# Tracing: "none" | "console" | "otlp"
LLM_TRACE_ENABLED=false
LLM_TRACE_EXPORTER=none
LLM_TRACE_ENDPOINT=http://localhost:4317
LLM_TRACE_SERVICE_NAME=llm-gateway

# Logging: "json" | "console"
LLM_LOG_LEVEL=INFO
LLM_LOG_FORMAT=json
